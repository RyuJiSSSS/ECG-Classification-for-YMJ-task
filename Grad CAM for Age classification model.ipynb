{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cultural-action",
   "metadata": {},
   "source": [
    "# Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" os \"\"\"\n",
    "import os\n",
    "\n",
    "\"\"\" torch \"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "\"\"\"tensor board\"\"\"\n",
    "import torchvision\n",
    "\n",
    "\n",
    "\"\"\"glob\"\"\"\n",
    "from glob import glob\n",
    "\n",
    "\"\"\" tqdm \"\"\"\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\"Pandas\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\" numpy \"\"\"\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "from PIL import Image\n",
    "\n",
    "\"\"\"JSON\"\"\"\n",
    "import json\n",
    "\n",
    "\"\"\"sklearn\"\"\"\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "\"\"\"seaborn\"\"\"\n",
    "import seaborn as sns\n",
    "\n",
    "\"\"\"scipy\"\"\"\n",
    "from scipy import io\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, ifft,fftfreq\n",
    "from scipy import stats\n",
    "\n",
    "\"\"\"SUMMARY\"\"\"\n",
    "from torchsummary import summary\n",
    "\n",
    "\"\"\"time\"\"\"\n",
    "import time\n",
    "\n",
    "\n",
    "\"\"\"PIL\"\"\"\n",
    "from PIL import Image\n",
    "\n",
    "\"\"\"pingouin\"\"\"\n",
    "import pingouin as pg\n",
    "\n",
    "import re\n",
    "import shutil\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3bf6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "from torch.autograd import Function\n",
    "\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-philadelphia",
   "metadata": {},
   "source": [
    "# Path init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-alarm",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_path = \"path\"\n",
    "os.chdir(cur_path)\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80c09e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_num = \n",
    "fig_save_path = 'path'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-wesley",
   "metadata": {},
   "source": [
    "# Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "\n",
    "#validation ratio\n",
    "validation_ratio = 0.1\n",
    "\n",
    "#learning rate\n",
    "lr = 0.001\n",
    " \n",
    "\n",
    "momentum = 0.5\n",
    "\n",
    "\n",
    "batch_size = 512    \n",
    "test_batch_size = 512\n",
    "\n",
    "\n",
    "epochs = 50\n",
    "no_cuda = False\n",
    "\n",
    "log_interval = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-cooper",
   "metadata": {},
   "source": [
    "# Set the seed and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers':0,'pin_memory':True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-swing",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECG_CNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ECG_CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(12, 48, kernel_size=5, stride=1 ),\n",
    "            nn.BatchNorm1d(48),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(48, 96, kernel_size=5, stride=1 ),\n",
    "            nn.BatchNorm1d(96),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(96, 192, kernel_size=5, stride=1 ),\n",
    "            nn.BatchNorm1d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        \"\"\"Sigmoid\"\"\"\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "        \"\"\" Global Max, Average Pooling\"\"\"\n",
    "        self.globalavrpool1 = nn.AvgPool1d(kernel_size = 621, stride=1)\n",
    "        \n",
    "        \"\"\"Drop out\"\"\"\n",
    "        self.dp = nn.Dropout(p=0.5)\n",
    "        \n",
    "        \n",
    "        \"\"\"Fully Connected \"\"\" \n",
    "        self.fc1 = nn.Linear(1*192, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.globalavrpool1(x)\n",
    "        \n",
    "        \"\"\"Fully Connected\"\"\"\n",
    "        x = x.view(-1, 1*192*1) \n",
    "        \n",
    "        # Dense\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-implementation",
   "metadata": {},
   "source": [
    "# Data Path(12-Lead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd55eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('path', 'r') as f:\n",
    "    label_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths_infant,data_paths_10s,data_paths_20s,data_paths_30s,data_paths_40s,data_paths_50s,data_paths_60s  = [],[],[],[],[],[],[]\n",
    "data_paths_70s, data_paths_80s, data_paths_90s = [],[],[]\n",
    "\n",
    "for i in range(0,len(label_data),1):\n",
    "    \n",
    "    \"\"\"get label\"\"\"\n",
    "    tempt = label_data[i]['age']\n",
    "    \n",
    "    if tempt<18:\n",
    "        data_paths_infant.append(i)\n",
    "    elif 18 <= tempt < 20:\n",
    "        data_paths_10s.append(i)\n",
    "    elif 20 <= tempt < 30:\n",
    "        data_paths_20s.append(i)\n",
    "    elif 30 <= tempt < 40:\n",
    "        data_paths_30s.append(i)\n",
    "    elif 40 <= tempt < 50:\n",
    "        data_paths_40s.append(i)\n",
    "    elif 50 <= tempt < 60:\n",
    "        data_paths_50s.append(i)\n",
    "    elif 60 <= tempt < 70:\n",
    "        data_paths_60s.append(i)\n",
    "    elif 70 <= tempt < 80:\n",
    "        data_paths_70s.append(i)\n",
    "    elif 80 <= tempt < 90:\n",
    "        data_paths_80s.append(i)\n",
    "    else:\n",
    "        data_paths_90s.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-shift",
   "metadata": {},
   "source": [
    "# train, validatin, test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_train_validation_test_paths(data_paths, train_ratio, valid_ratio):\n",
    "    \n",
    "    \"\"\"count number of data paths\"\"\"\n",
    "    num_datapaths = len(data_paths)\n",
    "    indices = list(range(num_datapaths))\n",
    "    \n",
    "    train_split = int(np.floor(train_ratio * num_datapaths))\n",
    "    valid_split = int(np.floor((train_ratio+valid_ratio) * num_datapaths))\n",
    "\n",
    "    \"\"\"set the seed and shuffle\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    \"\"\"spllit data paths\"\"\"\n",
    "    train_data_paths      = []\n",
    "    validation_data_paths = []\n",
    "    test_data_pahts       = []\n",
    "    \n",
    "    \n",
    "    train_idx, valid_idx, test_idx = indices[:train_split], indices[train_split:valid_split], indices[valid_split:]\n",
    "    \n",
    "\n",
    "    for i in range(0,len(train_idx),1):\n",
    "        train_data_paths.append( data_paths[train_idx[i]])\n",
    "        \n",
    "    for i in range(0,len(valid_idx),1):\n",
    "        validation_data_paths.append( data_paths[valid_idx[i]])\n",
    "        \n",
    "    for i in range(0,len(test_idx),1):\n",
    "        test_data_pahts.append( data_paths[test_idx[i]])\n",
    "    \n",
    "    return train_data_paths, validation_data_paths, test_data_pahts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths_10s, validation_paths_10s, test_paths_10s = Make_train_validation_test_paths(data_paths_10s,0.4,0.3)\n",
    "train_paths_20s, validation_paths_20s, test_paths_20s = Make_train_validation_test_paths(data_paths_20s,0.4,0.3)\n",
    "train_paths_30s, validation_paths_30s, test_paths_30s = Make_train_validation_test_paths(data_paths_30s,0.4,0.3)\n",
    "train_paths_40s, validation_paths_40s, test_paths_40s = Make_train_validation_test_paths(data_paths_40s,0.4,0.3)\n",
    "train_paths_50s, validation_paths_50s, test_paths_50s = Make_train_validation_test_paths(data_paths_50s,0.4,0.3)\n",
    "train_paths_60s, validation_paths_60s, test_paths_60s = Make_train_validation_test_paths(data_paths_60s,0.4,0.3)\n",
    "train_paths_70s, validation_paths_70s, test_paths_70s = Make_train_validation_test_paths(data_paths_70s,0.4,0.3)\n",
    "train_paths_80s, validation_paths_80s, test_paths_80s = Make_train_validation_test_paths(data_paths_80s,0.4,0.3)\n",
    "train_paths_90s, validation_paths_90s, test_paths_90s = Make_train_validation_test_paths(data_paths_90s,0.4,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-hungarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths      = train_paths_10s + train_paths_20s + train_paths_30s + train_paths_40s + train_paths_50s + train_paths_60s + train_paths_70s + train_paths_80s + train_paths_90s\n",
    "validation_paths = validation_paths_10s + validation_paths_20s + validation_paths_30s + validation_paths_40s + validation_paths_50s + validation_paths_60s + validation_paths_70s + validation_paths_80s + validation_paths_90s\n",
    "test_paths       = test_paths_10s + test_paths_20s + test_paths_30s + test_paths_40s + test_paths_50s + test_paths_60s + test_paths_70s + test_paths_80s + test_paths_90s\n",
    "len(train_paths), len(validation_paths), len(test_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-legislation",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-money",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_paths, transform=None):\n",
    "        self.data_paths = data_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "          \n",
    "        \"\"\"get data\"\"\"\n",
    "        tempt = self.data_paths[idx] \n",
    "        path  = label_data[tempt]['data_path']\n",
    "        data = np.load(file = path)\n",
    "        \n",
    "        \n",
    "        \"\"\"get label\"\"\"\n",
    "        \n",
    "        if label_data[tempt]['age'] <= 40:\n",
    "            label = 0\n",
    "        else:\n",
    "            label = 1\n",
    "        \n",
    "\n",
    "        return data,label\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset      = CustomDataset(train_paths,transforms.Compose([transforms.ToTensor()]))\n",
    "validation_dataset = CustomDataset(validation_paths,transforms.Compose([transforms.ToTensor()]))\n",
    "test_dataset       = CustomDataset(test_paths,transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "print(len(train_dataset),len(validation_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-norfolk",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-campus",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train\"\"\"\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,  \n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "\"\"\"Validation\"\"\"\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    dataset=validation_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "\"\"\"Test\"\"\"\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size = test_batch_size,\n",
    "    shuffle = True,\n",
    "    **kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of the train_loader:\", len(train_loader))\n",
    "print(\"Length of the val_loader:\", len(validation_loader))\n",
    "print(\"Length of the test_loader:\", len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-helping",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-asset",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ECG_CNN().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-carol",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-lithuania",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train\"\"\"\n",
    "train_losses        = []\n",
    "avg_train_losses    = []\n",
    "Train_baths_ACC     = [] \n",
    "Train_ACC           = [] \n",
    "\n",
    "\n",
    "\"\"\"Validaion\"\"\"\n",
    "valid_losses        = []\n",
    "avg_valid_losses    = []\n",
    "Validation_ACC      = []\n",
    "Valid_ACC_per_Class = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7b7c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"save best model\"\"\"\n",
    "best_acc = 0\n",
    "best_model_save_path = 'path'\n",
    "best_model_save_path = best_model_save_path + str(experiment_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3657b884",
   "metadata": {},
   "source": [
    "# Model save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9c193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 경로\n",
    "save_path = 'path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c369d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = save_path + str(experiment_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0500b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78939de",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4084e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76df0e54",
   "metadata": {},
   "source": [
    "# CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f08bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor():\n",
    "    \"\"\" Class for extracting activations and \n",
    "    registering gradients from targetted intermediate layers \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.gradients = []\n",
    "\n",
    "    def save_gradient(self, grad):\n",
    "        self.gradients.append(grad)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outputs = []\n",
    "        self.gradients = []\n",
    "        for name, module in self.model._modules.items():\n",
    "\n",
    "            if name == '3':\n",
    "                x = module(x)\n",
    "               \n",
    "            else:\n",
    "                x = module(x)\n",
    "                x.register_hook(self.save_gradient)\n",
    "                outputs += [x]\n",
    "        return outputs, x\n",
    "\n",
    "\n",
    "class ModelOutputs():\n",
    "    \"\"\" Class for making a forward pass, and getting:\n",
    "    1. The network output.\n",
    "    2. Activations from intermeddiate targetted layers.\n",
    "    3. Gradients from intermeddiate targetted layers. \"\"\"\n",
    "\n",
    "    def __init__(self, model, feature_module):\n",
    "        self.model = model\n",
    "        self.feature_module = feature_module\n",
    "        self.feature_extractor = FeatureExtractor(self.feature_module)\n",
    "\n",
    "    def get_gradients(self):\n",
    "        return self.feature_extractor.gradients\n",
    "\n",
    "    def __call__(self, x):\n",
    "        target_activations = []\n",
    "        for name, module in self.model._modules.items():\n",
    "            if module == self.feature_module:\n",
    "                target_activations, x = self.feature_extractor(x)\n",
    "            elif \"globalavrpool1\" in name.lower():\n",
    "                x = module(x)\n",
    "                x = x.view(-1, 192)\n",
    "            elif name == 'fc3':\n",
    "                x = F.softmax(module(x))\n",
    "            elif name == 'fc1' or name == 'fc2':\n",
    "                x = F.relu(module(x))\n",
    "            else:\n",
    "                x = module(x)\n",
    "        \n",
    "        return target_activations, x\n",
    "\n",
    "\n",
    "\n",
    "def interpolation(mask,shape):\n",
    "    x = np.linspace(0,10,shape)\n",
    "    y = mask\n",
    "    fq = interp1d(x,y,kind = 'quadratic')\n",
    "\n",
    "    xint = np.linspace(x.min(), x.max(), 5000)\n",
    "    yintq = fq(xint)\n",
    "    yintq = yintq - np.min(yintq)\n",
    "    yintq = yintq / (np.max(yintq) +0.00001)\n",
    "    return yintq\n",
    "\n",
    "class GradCam:\n",
    "    def __init__(self, model, feature_module, use_cuda):\n",
    "        self.model = model\n",
    "        self.feature_module = feature_module\n",
    "        self.model.eval()\n",
    "        self.cuda = use_cuda\n",
    "        if self.cuda:\n",
    "            self.model = model.cuda()\n",
    "\n",
    "        self.extractor = ModelOutputs(self.model, self.feature_module)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n",
    "    def __call__(self, input, index=None):\n",
    "        if self.cuda:\n",
    "            features, output = self.extractor(input.cuda())\n",
    "            \n",
    "        else:\n",
    "            features, output = self.extractor(input)\n",
    "\n",
    "        if index == None:\n",
    "            index = np.argmax(output.cpu().data.numpy())\n",
    "        \n",
    "        one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
    "        one_hot[0][index] = 1\n",
    "        one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
    "        \n",
    "\n",
    "        if self.cuda:\n",
    "            one_hot = torch.sum(one_hot.cuda() * output)\n",
    "        else:\n",
    "            one_hot = torch.sum(one_hot * output)\n",
    "\n",
    "        self.feature_module.zero_grad()\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        one_hot.backward(retain_graph=True)\n",
    "        grads_val = self.extractor.get_gradients()[-1].cpu().data.numpy()\n",
    "        target = features[-1]\n",
    "        target = target.cpu().data.numpy()[0, :]\n",
    "\n",
    "\n",
    "        weights = np.mean(grads_val, axis=(2))[0, :]\n",
    "        cam = np.zeros(target.shape[1:], dtype=np.float32)\n",
    "\n",
    "        for i, w in enumerate(weights):\n",
    "\n",
    "            cam += w * target[i, :]\n",
    "            \n",
    "        cam  = torch.from_numpy(cam)\n",
    "        cam = F.relu(cam)\n",
    "        cam =  cam.numpy()\n",
    "           \n",
    "        \n",
    "        cam = cam - np.min(cam)\n",
    "        cam = cam / (np.max(cam) +0.00001)\n",
    "        return cam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bfc922",
   "metadata": {},
   "source": [
    "# Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23901ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_save_path = 'path'\n",
    "best_model_save_path = best_model_save_path + str(experiment_num)\n",
    "\n",
    "best_model_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274eb6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb43566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(best_model_save_path)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ed8ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_path = 'path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09161b98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grad_cam = GradCam(model=model, feature_module=model.conv3, \\\n",
    "                use_cuda=True)\n",
    "model.eval()\n",
    "index = 0\n",
    "for dnn_inputs, dnn_target in test_loader:\n",
    "\n",
    "        \n",
    "    for i in range(len(dnn_inputs)):\n",
    "        print(i)\n",
    "        target_index = dnn_target[i]\n",
    "        grad_cam_input = dnn_inputs[i].unsqueeze(0).cuda()\n",
    "        plot_cam_input = dnn_inputs[i]\n",
    "        \n",
    "        output = model(grad_cam_input).cpu()\n",
    "        if output[0][1] > 0.5:\n",
    "            predicted_index = 1\n",
    "        else:\n",
    "            predicted_index = 0\n",
    "\n",
    "        index = index + 1\n",
    "        if target_index == predicted_index:\n",
    "            mask = grad_cam(grad_cam_input, target_index)\n",
    "            if mask.sum() == 0:\n",
    "                pass\n",
    "            else:\n",
    "\n",
    "                interpolted_mask = interpolation(mask,mask.shape[0])\n",
    "                a = np.append(np.expand_dims(interpolted_mask, axis=0), np.expand_dims(interpolted_mask, axis=0), axis=0)\n",
    "                for j in range(50):\n",
    "                    a = np.append(a,np.expand_dims(interpolted_mask, axis=0),axis = 0)\n",
    "                heatmap = cv2.applyColorMap(np.uint8(a*255), cv2.COLORMAP_JET)\n",
    "                heatmap = np.float32(heatmap)\n",
    "\n",
    "                f, axes = plt.subplots(12, 1, figsize=(12, 20))\n",
    "                x = np.arange(0,10,0.002)\n",
    "                for j in range(12):\n",
    "                    axes[j].plot(x, plot_cam_input[j],color = 'black')\n",
    "                    axes[j].set_xlim([0, 10])\n",
    "                    axes[j].axis('off')\n",
    "\n",
    "                f.tight_layout()\n",
    "                plt.savefig('./signal.jpg')\n",
    "                img = cv2.imread('./signal.jpg')\n",
    "                heatmap = cv2.resize(heatmap , dsize=(img.shape[1],img.shape[0]), interpolation=cv2.INTER_AREA)\n",
    "                \n",
    "                \n",
    "                #overlay\n",
    "                cam = cv2.addWeighted(np.float32(img),0.5,heatmap,0.75,0)\n",
    "                \n",
    "                # normalize\n",
    "                cam2 = cv2.normalize(cam, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "                # BGR 2 RGB\n",
    "                cam2 = cv2.cvtColor(cam2, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                \n",
    "                plt.imshow(cam2)\n",
    "                plt.axis('off')\n",
    "                \n",
    "                if target_index == 1:\n",
    "                     plt.imsave(cam_path+'/'+'1/'+str(index)+'.png', cam2, dpi=600)\n",
    "                    \n",
    "                else:\n",
    "                     plt.imsave(cam_path+'/'+'0/'+str(index)+'.png', cam2, dpi=600)\n",
    "                \n",
    "                \n",
    "                del f, axes\n",
    "        else:\n",
    "            pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

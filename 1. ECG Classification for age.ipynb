{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cultural-action",
   "metadata": {},
   "source": [
    "# Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" os \"\"\"\n",
    "import os\n",
    "\n",
    "\"\"\" torch \"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "\"\"\"tensor board\"\"\"\n",
    "import torchvision\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\"\"\"glob\"\"\"\n",
    "from glob import glob\n",
    "\n",
    "\"\"\" tqdm \"\"\"\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\"Pandas\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\" numpy \"\"\"\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "from PIL import Image\n",
    "\n",
    "\"\"\"JSON\"\"\"\n",
    "import json\n",
    "\n",
    "\"\"\"sklearn\"\"\"\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "\"\"\"seaborn\"\"\"\n",
    "import seaborn as sns\n",
    "\n",
    "\"\"\"scipy\"\"\"\n",
    "from scipy import io\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, ifft,fftfreq\n",
    "from scipy import stats\n",
    "\n",
    "\"\"\"SUMMARY\"\"\"\n",
    "from torchsummary import summary\n",
    "\n",
    "\"\"\"time\"\"\"\n",
    "import time\n",
    "\n",
    "\n",
    "\"\"\"pingouin\"\"\"\n",
    "import pingouin as pg\n",
    "\n",
    "import re\n",
    "import shutil\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-philadelphia",
   "metadata": {},
   "source": [
    "# Path init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b38f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-alarm",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_path = \"path\"\n",
    "os.chdir(cur_path)\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80c09e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_num = \n",
    "fig_save_path = 'path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e1bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = fig_save_path + str(experiment_num)\n",
    "\n",
    "try:\n",
    "    if not(os.path.isdir(folder_path)):\n",
    "        os.makedirs(os.path.join(folder_path))\n",
    "except OSError as e:\n",
    "    if e.errno != errno.EEXIST:\n",
    "        print(\"Failed to create directory!!!!!\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-wesley",
   "metadata": {},
   "source": [
    "# Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "\n",
    "#validation ratio\n",
    "validation_ratio = 0.1\n",
    "\n",
    "#learning rate\n",
    "lr = 0.001\n",
    " \n",
    "\n",
    "momentum = 0.5\n",
    "\n",
    "\n",
    "batch_size = 512    \n",
    "test_batch_size = 512\n",
    "\n",
    "\n",
    "epochs = 50\n",
    "no_cuda = False\n",
    "\n",
    "log_interval = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-cooper",
   "metadata": {},
   "source": [
    "# Set the seed and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers':0,'pin_memory':True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-swing",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECG_CNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ECG_CNN, self).__init__()\n",
    "\n",
    "        \"\"\"Convolution\"\"\"\n",
    "        self.conv1 = nn.Conv1d(12, 48, kernel_size=5, stride=1 )\n",
    "        self.conv2 = nn.Conv1d(48, 96, kernel_size=5, stride=1 )\n",
    "        self.conv3 = nn.Conv1d(96, 192, kernel_size=5, stride=1 )\n",
    "        \n",
    "        \"\"\"BatchNormalize\"\"\"\n",
    "        self.bn1 = nn.BatchNorm1d(48)\n",
    "        self.bn2 = nn.BatchNorm1d(96)\n",
    "        self.bn3 = nn.BatchNorm1d(192)\n",
    "        \n",
    "        \"\"\"ReLU\"\"\"\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        \"\"\"Sigmoid\"\"\"\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \"\"\" MaxPooling\"\"\"     \n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        \"\"\" Global Max, Average Pooling\"\"\"\n",
    "        self.globalavrpool1 = nn.AvgPool1d(kernel_size = 621, stride=1)\n",
    "        \n",
    "        \"\"\"Drop out\"\"\"\n",
    "        self.dp = nn.Dropout(p=0.5)\n",
    "        \n",
    "        \n",
    "        \"\"\"Fully Connected \"\"\" \n",
    "        self.fc1 = nn.Linear(1*192, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.globalavrpool1(x)\n",
    "        \n",
    "        \n",
    "        \"\"\"Fully Connected\"\"\"\n",
    "        x = x.view(-1, 1*192*1) .\n",
    "        \n",
    "        \n",
    "        # Dense\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-implementation",
   "metadata": {},
   "source": [
    "# Data Path(12 Lead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd55eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('path', 'r') as f:\n",
    "    label_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-tractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths_infant,data_paths_10s,data_paths_20s,data_paths_30s,data_paths_40s,data_paths_50s,data_paths_60s  = [],[],[],[],[],[],[]\n",
    "data_paths_70s, data_paths_80s, data_paths_90s = [],[],[]\n",
    "\n",
    "for i in range(0,len(label_data),1):\n",
    "    \n",
    "    \"\"\"get label\"\"\"\n",
    "    tempt = label_data[i]['age']\n",
    "    \n",
    "    if tempt<18:\n",
    "        data_paths_infant.append(i)\n",
    "    elif 18 <= tempt < 20:\n",
    "        data_paths_10s.append(i)\n",
    "    elif 20 <= tempt < 30:\n",
    "        data_paths_20s.append(i)\n",
    "    elif 30 <= tempt < 40:\n",
    "        data_paths_30s.append(i)\n",
    "    elif 40 <= tempt < 50:\n",
    "        data_paths_40s.append(i)\n",
    "    elif 50 <= tempt < 60:\n",
    "        data_paths_50s.append(i)\n",
    "    elif 60 <= tempt < 70:\n",
    "        data_paths_60s.append(i)\n",
    "    elif 70 <= tempt < 80:\n",
    "        data_paths_70s.append(i)\n",
    "    elif 80 <= tempt < 90:\n",
    "        data_paths_80s.append(i)\n",
    "    else:\n",
    "        data_paths_90s.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-shift",
   "metadata": {},
   "source": [
    "# train, validatin, test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_train_validation_test_paths(data_paths, train_ratio, valid_ratio):\n",
    "    \n",
    "    \"\"\"count number of data paths\"\"\"\n",
    "    num_datapaths = len(data_paths)\n",
    "    indices = list(range(num_datapaths))\n",
    "    \n",
    "    train_split = int(np.floor(train_ratio * num_datapaths))\n",
    "    valid_split = int(np.floor((train_ratio+valid_ratio) * num_datapaths))\n",
    "\n",
    "    \"\"\"set the seed and shuffle\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    \"\"\"spllit data paths\"\"\"\n",
    "    train_data_paths      = []\n",
    "    validation_data_paths = []\n",
    "    test_data_pahts       = []\n",
    "    \n",
    "    \n",
    "    train_idx, valid_idx, test_idx = indices[:train_split], indices[train_split:valid_split], indices[valid_split:]\n",
    "    \n",
    "\n",
    "    for i in range(0,len(train_idx),1):\n",
    "        train_data_paths.append( data_paths[train_idx[i]])\n",
    "        \n",
    "    for i in range(0,len(valid_idx),1):\n",
    "        validation_data_paths.append( data_paths[valid_idx[i]])\n",
    "        \n",
    "    for i in range(0,len(test_idx),1):\n",
    "        test_data_pahts.append( data_paths[test_idx[i]])\n",
    "        \n",
    "    \n",
    "    \n",
    "    return train_data_paths, validation_data_paths, test_data_pahts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths_10s, validation_paths_10s, test_paths_10s = Make_train_validation_test_paths(data_paths_10s,0.4,0.3)\n",
    "train_paths_20s, validation_paths_20s, test_paths_20s = Make_train_validation_test_paths(data_paths_20s,0.4,0.3)\n",
    "train_paths_30s, validation_paths_30s, test_paths_30s = Make_train_validation_test_paths(data_paths_30s,0.4,0.3)\n",
    "train_paths_40s, validation_paths_40s, test_paths_40s = Make_train_validation_test_paths(data_paths_40s,0.4,0.3)\n",
    "train_paths_50s, validation_paths_50s, test_paths_50s = Make_train_validation_test_paths(data_paths_50s,0.4,0.3)\n",
    "train_paths_60s, validation_paths_60s, test_paths_60s = Make_train_validation_test_paths(data_paths_60s,0.4,0.3)\n",
    "train_paths_70s, validation_paths_70s, test_paths_70s = Make_train_validation_test_paths(data_paths_70s,0.4,0.3)\n",
    "train_paths_80s, validation_paths_80s, test_paths_80s = Make_train_validation_test_paths(data_paths_80s,0.4,0.3)\n",
    "train_paths_90s, validation_paths_90s, test_paths_90s = Make_train_validation_test_paths(data_paths_90s,0.4,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-hungarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths      = train_paths_10s + train_paths_20s + train_paths_30s + train_paths_40s + train_paths_50s + train_paths_60s + train_paths_70s + train_paths_80s + train_paths_90s\n",
    "validation_paths = validation_paths_10s + validation_paths_20s + validation_paths_30s + validation_paths_40s + validation_paths_50s + validation_paths_60s + validation_paths_70s + validation_paths_80s + validation_paths_90s\n",
    "test_paths       = test_paths_10s + test_paths_20s + test_paths_30s + test_paths_40s + test_paths_50s + test_paths_60s + test_paths_70s + test_paths_80s + test_paths_90s\n",
    "len(train_paths), len(validation_paths), len(test_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-legislation",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-money",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_paths, transform=None):\n",
    "        self.data_paths = data_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "          \n",
    "        \"\"\"get data\"\"\"\n",
    "        tempt = self.data_paths[idx] \n",
    "        path  = label_data[tempt]['data_path']\n",
    "        data = np.load(file = path)\n",
    "        \n",
    "        \n",
    "        \"\"\"get label\"\"\"\n",
    "        \n",
    "        if label_data[tempt]['age'] <= 40:\n",
    "            label = 0\n",
    "        else:\n",
    "            label = 1\n",
    "        \n",
    "\n",
    "        return data,label\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset      = CustomDataset(train_paths,transforms.Compose([transforms.ToTensor()]))\n",
    "validation_dataset = CustomDataset(validation_paths,transforms.Compose([transforms.ToTensor()]))\n",
    "test_dataset       = CustomDataset(test_paths,transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "print(len(train_dataset),len(validation_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-norfolk",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-campus",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train\"\"\"\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    #sampler = train_sampler,\n",
    "    shuffle = True, \n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "\"\"\"Validation\"\"\"\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    dataset=validation_dataset,\n",
    "    batch_size = batch_size,\n",
    "    #sampler = valid_sampler,\n",
    "    shuffle = True,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "\"\"\"Test\"\"\"\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size = test_batch_size,\n",
    "    shuffle = True,\n",
    "    **kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of the train_loader:\", len(train_loader))\n",
    "print(\"Length of the val_loader:\", len(validation_loader))\n",
    "print(\"Length of the test_loader:\", len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-helping",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-asset",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ECG_CNN().to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-carol",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-lithuania",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train\"\"\"\n",
    "train_losses        = []\n",
    "avg_train_losses    = []\n",
    "Train_baths_ACC     = [] \n",
    "Train_ACC           = [] \n",
    "\n",
    "\n",
    "\"\"\"Validaion\"\"\"\n",
    "valid_losses        = []\n",
    "avg_valid_losses    = []\n",
    "Validation_ACC      = []\n",
    "Valid_ACC_per_Class = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7b7c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"save best model\"\"\"\n",
    "best_acc = 0\n",
    "best_model_save_path = 'path'\n",
    "best_model_save_path = best_model_save_path + str(experiment_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-incidence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    \"\"\"Train\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        data, target = data.to(device, dtype=torch.float), target.to(device, dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        #print(output)\n",
    "\n",
    "        \"\"\"pred(Cross Entropy)\"\"\"\n",
    "        #pred = output.argmax(dim=1, keepdim=True)  #cross entropy\n",
    "        pred = F.softmax(output,dim =1).argmax(dim=1, keepdim=True)\n",
    "        \n",
    "        \"\"\"loss\"\"\"\n",
    "        loss = criterion(output, target)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        correct = 0\n",
    "        total = target.size(0)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        accuracy = 100. * correct / total\n",
    "        Train_baths_ACC.append(accuracy)\n",
    "        \n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            #1.\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "            #2.\n",
    "            print('Train set:  batch loss: {:.4f}, Accuracy: {:.0f}% '.format(\n",
    "                loss.item() ,accuracy))\n",
    "\n",
    "    \n",
    "    \"\"\"Validation\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    valid_loss = 0\n",
    "    correct = 0\n",
    "    total = len(validation_loader.dataset)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(validation_loader):\n",
    "            data, target = data.to(device, dtype=torch.float), target.to(device, dtype=torch.long)\n",
    "            \n",
    "            output = model(data)\n",
    "\n",
    "\n",
    "            \"\"\"pred(Cross Entropy)\"\"\"\n",
    "            #pred = output.argmax(dim=1, keepdim=True) # cross entropy\n",
    "            pred = F.softmax(output,dim =1).argmax(dim=1, keepdim=True) \n",
    "            \n",
    "            \"\"\"loss\"\"\"\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss += loss.item()\n",
    "        \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "\n",
    "                \n",
    "    \"\"\"Loss and ACC \"\"\"\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    valid_loss /= len(validation_loader)\n",
    "    avg_train_losses.append(train_loss)\n",
    "    avg_valid_losses.append(valid_loss)\n",
    "    \n",
    "    Train_ACC.append(sum(Train_baths_ACC)/len(Train_baths_ACC))\n",
    "    Train_baths_ACC = []\n",
    "    \n",
    "    valid_accuracy = 100. * correct / total\n",
    "    Validation_ACC.append(valid_accuracy)\n",
    "\n",
    "    \n",
    "    print('------------------------------------------')\n",
    "    print('Valid set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        valid_loss, correct, total, valid_accuracy))\n",
    "    print('-------------------------------------------')\n",
    "    \n",
    "    \n",
    "    \"\"\"Save best model\"\"\"\n",
    "    \n",
    "    if valid_accuracy > best_acc:\n",
    "        torch.save(model, best_model_save_path)\n",
    "        print(\"model saved.\")\n",
    "        print('-------------------------------------------')\n",
    "        best_acc = valid_accuracy\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3657b884",
   "metadata": {},
   "source": [
    "# Model save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9c193",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c369d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = save_path + str(experiment_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0500b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfc937b",
   "metadata": {},
   "source": [
    "# Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b82a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(best_model_save_path)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1832c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbc2a90",
   "metadata": {},
   "source": [
    "# Youden index's cut off "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b0ab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels  = np.array([]) \n",
    "pred_labels  = np.array([]) \n",
    "target_score = np.array([]) \n",
    "\n",
    "\"\"\"Validation\"\"\"\n",
    "model.eval()\n",
    "    \n",
    "valid_loss = 0\n",
    "correct = 0\n",
    "total = len(validation_loader.dataset)\n",
    "    \n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(validation_loader):\n",
    "        data, target = data.to(device, dtype=torch.float), target.to(device, dtype=torch.long)\n",
    "        output = model(data)\n",
    "        \n",
    "        \"\"\"pred(Cross Entropy)\"\"\"     \n",
    "        output = F.softmax(output,dim=1)\n",
    "        pred   = output.argmax(dim=1, keepdim=True)\n",
    "        \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        \n",
    "\n",
    "        true_labels = np.append(true_labels,np.array(target.cpu()))\n",
    "        pred_labels = np.append(pred_labels,np.array(pred.cpu()))  \n",
    "        target_score = np.append(target_score,np.array(output[:,1].cpu())) \n",
    "        \n",
    "        print('validation: [{}/{}] '.format(batch_idx,len(validation_loader)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d80293",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Youden’s J statistic. / J = Sensitivity + Specificity – 1\"\"\"\n",
    "\n",
    "# calculate roc curves\n",
    "FPR, TPR, thresholds = roc_curve(true_labels, target_score)\n",
    "\n",
    "# get the best threshold\n",
    "J = TPR - FPR\n",
    "idx = argmax(J)\n",
    "best_thresh = thresholds[idx]\n",
    "\n",
    "print('Best Threshold=%f, sensitivity = %.3f, specificity = %.3f, J=%.3f' % (best_thresh, TPR[idx], 1-FPR[idx], J[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71051c60",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee204156",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels  = np.array([]) \n",
    "pred_labels  = np.array([]) \n",
    "target_score = np.array([]) \n",
    "\n",
    "\"\"\"test\"\"\"\n",
    "model.eval()\n",
    "    \n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = len(test_loader.dataset)\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        \n",
    "        \n",
    "        data, target = data.to(device, dtype=torch.float), target.to(device, dtype=torch.long)\n",
    "        output = model(data)\n",
    "        \n",
    "        \"\"\"pred\"\"\"\n",
    "        output = F.softmax(output,dim=1)\n",
    "        \n",
    "        pred = (output[:,1] > best_thresh).int() \n",
    "        \n",
    "        \n",
    "        \"\"\"loss\"\"\"\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        \n",
    "        true_labels = np.append(true_labels,np.array(target.cpu())) \n",
    "        pred_labels = np.append(pred_labels,np.array(pred.cpu()))  \n",
    "        target_score = np.append(target_score,np.array(output[:,1].cpu())) \n",
    "        \n",
    "        print('test: [{}/{}] '.format(batch_idx,len(test_loader)-1))\n",
    "        \n",
    "        \n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = 100. * correct / total\n",
    "\n",
    "\n",
    "print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "    test_loss, correct, total, test_accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cultural-action",
   "metadata": {},
   "source": [
    "# Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" os \"\"\"\n",
    "import os\n",
    "\n",
    "\"\"\" torch \"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "\"\"\"tensor board\"\"\"\n",
    "import torchvision\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\"\"\"glob\"\"\"\n",
    "from glob import glob\n",
    "\n",
    "\"\"\" tqdm \"\"\"\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\"Pandas\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\" numpy \"\"\"\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "from PIL import Image\n",
    "\n",
    "\"\"\"JSON\"\"\"\n",
    "import json\n",
    "\n",
    "\"\"\"sklearn\"\"\"\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "\"\"\"seaborn\"\"\"\n",
    "import seaborn as sns\n",
    "\n",
    "\"\"\"scipy\"\"\"\n",
    "from scipy import io\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, ifft,fftfreq\n",
    "from scipy import stats\n",
    "\n",
    "\"\"\"SUMMARY\"\"\"\n",
    "from torchsummary import summary\n",
    "\n",
    "\"\"\"time\"\"\"\n",
    "import time\n",
    "\n",
    "\n",
    "\"\"\"pingouin\"\"\"\n",
    "import pingouin as pg\n",
    "\n",
    "import re\n",
    "import shutil\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-philadelphia",
   "metadata": {},
   "source": [
    "# Path init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-alarm",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_path = \"path\"\n",
    "os.chdir(cur_path)\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fa88de",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_num = \n",
    "fig_save_path = 'path'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-wesley",
   "metadata": {},
   "source": [
    "# Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "\n",
    "#validation ratio\n",
    "validation_ratio = 0.1\n",
    "\n",
    "#learning rate\n",
    "lr = 0.001\n",
    " \n",
    "momentum = 0.5\n",
    "\n",
    "\n",
    "batch_size = 512    \n",
    "test_batch_size = 512\n",
    "\n",
    "\n",
    "epochs = 50\n",
    "no_cuda = False\n",
    "\n",
    "log_interval = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-cooper",
   "metadata": {},
   "source": [
    "# Set the seed and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers':0,'pin_memory':True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-swing",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECG_CNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ECG_CNN, self).__init__()\n",
    "\n",
    "        \"\"\"Convolution\"\"\"\n",
    "        self.conv1 = nn.Conv1d(12, 48, kernel_size=5, stride=1 )\n",
    "        self.conv2 = nn.Conv1d(48, 96, kernel_size=5, stride=1 )\n",
    "        self.conv3 = nn.Conv1d(96, 192, kernel_size=5, stride=1 )\n",
    "        \n",
    "        \"\"\"BatchNormalize\"\"\"\n",
    "        self.bn1 = nn.BatchNorm1d(48)\n",
    "        self.bn2 = nn.BatchNorm1d(96)\n",
    "        self.bn3 = nn.BatchNorm1d(192)\n",
    "        \n",
    "        \"\"\"ReLU\"\"\"\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        \"\"\"Sigmoid\"\"\"\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \"\"\" MaxPooling\"\"\"     \n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        \"\"\" Global Max, Average Pooling\"\"\"\n",
    "        #self.globalmaxpool1 = nn.MaxPool1d(246, stride=1)\n",
    "        self.globalavrpool1 = nn.AvgPool1d(kernel_size = 621, stride=1)\n",
    "        \n",
    "        \"\"\"Drop out\"\"\"\n",
    "        self.dp = nn.Dropout(p=0.5)\n",
    "        \n",
    "        \n",
    "        \"\"\"Fully Connected \"\"\" \n",
    "        self.fc1 = nn.Linear(1*192, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        #x = self.globalmaxpool1(x)\n",
    "        x = self.globalavrpool1(x)\n",
    "        \n",
    "        \n",
    "        \"\"\"Fully Connected\"\"\"\n",
    "        x = x.view(-1, 1*192*1) \n",
    "        \n",
    "   \n",
    "        \n",
    "        # Dense\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-implementation",
   "metadata": {},
   "source": [
    "# Data Path(12-Lead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6834b30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('path', 'r') as f:\n",
    "    label_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths_A    = []\n",
    "data_paths_B    = []\n",
    "data_paths_O    = []\n",
    "data_paths_AB   = []\n",
    "for i in range(0,len(label_data),1):\n",
    "    if label_data[i]['ABO'] == 'A':\n",
    "        data_paths_A.append(i)\n",
    "    elif label_data[i]['ABO'] == 'B':\n",
    "        data_paths_B.append(i)\n",
    "    elif label_data[i]['ABO'] == 'O':\n",
    "        data_paths_O.append(i)\n",
    "    elif label_data[i]['ABO'] == 'AB':\n",
    "        data_paths_AB.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-trade",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_paths_A), len(data_paths_B), len(data_paths_O), len(data_paths_AB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-shift",
   "metadata": {},
   "source": [
    "# train, validatin, test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_train_validation_test_paths(data_paths, train_ratio, valid_ratio):\n",
    "    \n",
    "    \"\"\"count number of data paths\"\"\"\n",
    "    num_datapaths = len(data_paths)\n",
    "    indices = list(range(num_datapaths))\n",
    "    \n",
    "    train_split = int(np.floor(train_ratio * num_datapaths))\n",
    "    valid_split = int(np.floor((train_ratio+valid_ratio) * num_datapaths))\n",
    "\n",
    "    \"\"\"set the seed and shuffle\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    \"\"\"spllit data paths\"\"\"\n",
    "    train_data_paths      = []\n",
    "    validation_data_paths = []\n",
    "    test_data_pahts       = []\n",
    "    \n",
    "    \n",
    "    train_idx, valid_idx, test_idx = indices[:train_split], indices[train_split:valid_split], indices[valid_split:]\n",
    "    \n",
    "\n",
    "    for i in range(0,len(train_idx),1):\n",
    "        train_data_paths.append( data_paths[train_idx[i]])\n",
    "        \n",
    "    for i in range(0,len(valid_idx),1):\n",
    "        validation_data_paths.append( data_paths[valid_idx[i]])\n",
    "        \n",
    "    for i in range(0,len(test_idx),1):\n",
    "        test_data_pahts.append( data_paths[test_idx[i]])\n",
    "        \n",
    "    \n",
    "    \n",
    "    return train_data_paths, validation_data_paths, test_data_pahts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-senator",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths_A, validation_paths_A, test_paths_A = Make_train_validation_test_paths(data_paths_A,0.4,0.3)\n",
    "train_paths_B, validation_paths_B, test_paths_B = Make_train_validation_test_paths(data_paths_B,0.4,0.3)\n",
    "train_paths_O, validation_paths_O, test_paths_O = Make_train_validation_test_paths(data_paths_O,0.4,0.3)\n",
    "train_paths_AB, validation_paths_AB, test_paths_AB = Make_train_validation_test_paths(data_paths_AB,0.4,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-second",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_paths_A), len(validation_paths_A), len(test_paths_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths      = train_paths_A + train_paths_B + train_paths_O + train_paths_AB\n",
    "validation_paths = validation_paths_A + validation_paths_B + validation_paths_O + validation_paths_AB\n",
    "test_paths       = test_paths_A + test_paths_B + test_paths_O + test_paths_AB\n",
    "len(train_paths), len(validation_paths), len(test_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-legislation",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "Class2Idx = {'A':np.int8(0), 'B':np.int8(0), 'O':np.int8(0), 'AB':np.int8(1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-money",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_paths, transform=None):\n",
    "        self.data_paths = data_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "          \n",
    "        \"\"\"get data\"\"\"\n",
    "        tempt = self.data_paths[idx] \n",
    "        path  = label_data[tempt]['data_path']\n",
    "        data = np.load(file = path)\n",
    "        \n",
    "        \n",
    "        \"\"\"get label\"\"\"\n",
    "        label = Class2Idx[ label_data[tempt]['ABO'] ]\n",
    "\n",
    "        return data,label\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset      = CustomDataset(train_paths,transforms.Compose([transforms.ToTensor()]))\n",
    "validation_dataset = CustomDataset(validation_paths,transforms.Compose([transforms.ToTensor()]))\n",
    "test_dataset       = CustomDataset(test_paths,transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "print(len(train_dataset),len(validation_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-norfolk",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train\"\"\"\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    #sampler = train_sampler,\n",
    "    shuffle = True, \n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "\"\"\"Validation\"\"\"\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    dataset=validation_dataset,\n",
    "    batch_size = batch_size,\n",
    "    #sampler = valid_sampler,\n",
    "    shuffle = True,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "\"\"\"Test\"\"\"\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size = test_batch_size,\n",
    "    shuffle = True,\n",
    "    **kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed7334a",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-asset",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ECG_CNN().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95d6b02",
   "metadata": {},
   "source": [
    "summary(model, (12, 5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-carol",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-lithuania",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train\"\"\"\n",
    "train_losses        = []\n",
    "avg_train_losses    = []\n",
    "Train_baths_ACC     = [] \n",
    "Train_ACC           = [] \n",
    "\n",
    "\n",
    "\"\"\"Validaion\"\"\"\n",
    "valid_losses        = []\n",
    "avg_valid_losses    = []\n",
    "Validation_ACC      = []\n",
    "Valid_ACC_per_Class = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7d376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"save best model\"\"\"\n",
    "best_acc = 0\n",
    "best_model_save_path = 'path'\n",
    "best_model_save_path = best_model_save_path + str(experiment_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-incidence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    \"\"\"Train\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        data, target = data.to(device, dtype=torch.float), target.to(device, dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        #print(output)\n",
    "\n",
    "        \"\"\"pred(Cross Entropy)\"\"\"\n",
    "\n",
    "        pred = F.softmax(output,dim =1).argmax(dim=1, keepdim=True) \n",
    "        \n",
    "        \"\"\"loss\"\"\"\n",
    "        loss = criterion(output, target)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        correct = 0\n",
    "        total = target.size(0)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        accuracy = 100. * correct / total\n",
    "        Train_baths_ACC.append(accuracy)\n",
    "        \n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            #1.\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "            #2.\n",
    "            print('Train set:  batch loss: {:.4f}, Accuracy: {:.0f}% '.format(\n",
    "                loss.item() ,accuracy))\n",
    "\n",
    "   \n",
    "    \n",
    "    \n",
    "    \"\"\"Validation\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    valid_loss = 0\n",
    "    correct = 0\n",
    "    total = len(validation_loader.dataset)\n",
    "    \n",
    "    valid_confusion_matrix = torch.zeros(num_classes, num_classes)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(validation_loader):\n",
    "            data, target = data.to(device, dtype=torch.float), target.to(device, dtype=torch.long)\n",
    "            \n",
    "            output = model(data)\n",
    "\n",
    "\n",
    "            \"\"\"pred(Cross Entropy)\"\"\"\n",
    "            #pred = output.argmax(dim=1, keepdim=True) # cross entropy\n",
    "            pred = F.softmax(output,dim =1).argmax(dim=1, keepdim=True) \n",
    "            \n",
    "            \"\"\"loss\"\"\"\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss += loss.item()\n",
    "        \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "            for t, p in zip(target.view(-1), pred.view(-1)):\n",
    "                valid_confusion_matrix[t.int(), p.int()] += 1\n",
    "\n",
    "                \n",
    "    \"\"\"Loss and ACC \"\"\"\n",
    "    \n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    valid_loss /= len(validation_loader)\n",
    "    avg_train_losses.append(train_loss)\n",
    "    avg_valid_losses.append(valid_loss)\n",
    "\n",
    "    Train_ACC.append(sum(Train_baths_ACC)/len(Train_baths_ACC))\n",
    "    Train_baths_ACC = []\n",
    "    \n",
    "    valid_accuracy = 100. * correct / total\n",
    "    Validation_ACC.append(valid_accuracy)\n",
    "    \n",
    "\n",
    "    Valid_ACC_per_Class.append((valid_confusion_matrix.diag()/valid_confusion_matrix.sum(1))*100)\n",
    "    \n",
    "    print('------------------------------------------')\n",
    "    print('Valid set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        valid_loss, correct, total, valid_accuracy))\n",
    "    print('-------------------------------------------')\n",
    "    \n",
    "    \n",
    "    \"\"\"Save best model\"\"\"\n",
    "    \n",
    "    if valid_accuracy > best_acc:\n",
    "        torch.save(model, best_model_save_path)\n",
    "        print(\"model saved.\")\n",
    "        print('-------------------------------------------')\n",
    "        best_acc = valid_accuracy\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac53fbec",
   "metadata": {},
   "source": [
    "# Model save and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f771655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c29fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84444ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = save_path + str(experiment_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782adf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd3d39d",
   "metadata": {},
   "source": [
    "# Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d048fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(best_model_save_path)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9860ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2e091b",
   "metadata": {},
   "source": [
    "# Youden index's cut off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a2d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels  = np.array([]) \n",
    "pred_labels  = np.array([]) \n",
    "target_score = np.array([]) \n",
    "\n",
    "\"\"\"Validation\"\"\"\n",
    "model.eval()\n",
    "    \n",
    "valid_loss = 0\n",
    "correct = 0\n",
    "total = len(validation_loader.dataset)\n",
    "    \n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(validation_loader):\n",
    "        data, target = data.to(device, dtype=torch.float), target.to(device, dtype=torch.long)\n",
    "        output = model(data)\n",
    "        \n",
    "        \"\"\"pred(Cross Entropy)\"\"\"     \n",
    "        output = F.softmax(output,dim=1)\n",
    "        pred   = output.argmax(dim=1, keepdim=True)\n",
    "        \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        \n",
    "        true_labels = np.append(true_labels,np.array(target.cpu())) \n",
    "        pred_labels = np.append(pred_labels,np.array(pred.cpu()))  \n",
    "        target_score = np.append(target_score,np.array(output[:,1].cpu())) \n",
    "        \n",
    "        print('validation: [{}/{}] '.format(batch_idx,len(validation_loader)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ab3251",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Youden’s J statistic. / J = Sensitivity + Specificity – 1\"\"\"\n",
    "\n",
    "# calculate roc curves\n",
    "FPR, TPR, thresholds = roc_curve(true_labels, target_score)\n",
    "\n",
    "# get the best threshold\n",
    "J = TPR - FPR\n",
    "idx = argmax(J)\n",
    "best_thresh = thresholds[idx]\n",
    "\n",
    "print('Best Threshold=%f, sensitivity = %.3f, specificity = %.3f, J=%.3f' % (best_thresh, TPR[idx], 1-FPR[idx], J[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57fe03d",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4acf62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "true_labels  = np.array([]) \n",
    "pred_labels  = np.array([]) \n",
    "target_score = np.array([]) \n",
    "\n",
    "\"\"\"test\"\"\"\n",
    "model.eval()\n",
    "    \n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = len(test_loader.dataset)\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        \n",
    "        \n",
    "        data, target = data.to(device, dtype=torch.float), target.to(device, dtype=torch.long)\n",
    "        output = model(data)\n",
    "        \n",
    "        \"\"\"pred\"\"\"\n",
    "        output = F.softmax(output,dim=1)\n",
    "        \n",
    "        #pred   = output.argmax(dim=1, keepdim=True)\n",
    "        pred = (output[:,1] > best_thresh).int() \n",
    "        \n",
    "        \n",
    "        \"\"\"loss\"\"\"\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"결과값 누적.\"\"\"\n",
    "        true_labels = np.append(true_labels,np.array(target.cpu())) \n",
    "        pred_labels = np.append(pred_labels,np.array(pred.cpu()))  \n",
    "        target_score = np.append(target_score,np.array(output[:,1].cpu())) \n",
    "        \n",
    "        print('test: [{}/{}] '.format(batch_idx,len(test_loader)-1))\n",
    "        \n",
    "        \n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = 100. * correct / total\n",
    "\n",
    "\n",
    "print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "    test_loss, correct, total, test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d2ccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
